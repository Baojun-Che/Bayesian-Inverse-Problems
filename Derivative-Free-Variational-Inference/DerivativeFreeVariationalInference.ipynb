{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative Free-Gaussian Mixture Variational Inference (DF-GMVI)\n",
    "This notebook is a brief introduction to the DF-GMVI algorithm proposed in the paper [\"Stable Derivative Free Gaussian Mixture Variational Inference for Bayesian Inverse Problems\"]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Natural Gradient Variational Inference\n",
    "Consider a posterior distribution $\\rho_{\\text{post}}(\\theta)\\propto \\exp(-\\Phi_R)(\\theta)$, where $\\Phi_R(\\theta)=\\frac{1}{2}\\mathcal{F}(\\theta)^T\\mathcal{F}(\\theta)$ and a variational family of densities $\\{ \\rho_a \\}$ parameterized by $a$. To minimize the KL divergence $\\text{KL}(\\rho_a||\\rho_{\\text{post}})$ over the variational family, we intoduct a general metric $\\langle\\cdot, \\mathfrak{M}(a) \\cdot \\rangle$, and the steepest descent direction becomes\n",
    "$$ -\\mathfrak{M}(a)^{-1}\\nabla_a  \\text{KL}[a \\Vert \\rho_{\\text{post}}] = \\argmin_\\sigma \\frac{\\langle \\nabla_a \\text{KL}[\\rho_{a} \\Vert \\rho_{\\text{post}}] ,  \\sigma\\rangle}{\\sqrt{\\langle \\sigma,  \\mathfrak{M}(a) \\sigma \\rangle}}.$$\n",
    "Now we focuses on the Fisher-Rao metric, where the metric tensor is the Fisher information matrix\n",
    "$$\\mathfrak{M}(a) = \\mathrm{FIM} (a):= \\int  \\frac{\\partial \\log \\rho_{a}(\\theta)}{\\partial a} \\frac{\\partial \\log \\rho_{a}(\\theta)}{\\partial a}^T \\rho_a(\\theta) \\,\\mathrm{d} \\theta.$$\n",
    "This  gives rise to the concept of natural gradient descent or natural variational inference, which corresponds to the following gradient flow: \n",
    "$$  \\frac{\\mathrm{d} a}{\\mathrm{d} t} = -\\mathrm{FIM}(a)^{-1}\\nabla_a  \\mathrm{KL}[\\rho_a \\Vert \\rho_ {\\text{post}}]. $$\n",
    "### Gaussian Approximation\n",
    "For Gaussian distribution family $rho_a(\\theta)=\\mathcal{N}(\\theta; m, C)$, where $a=[m,C]$, the Fisher information matrix is given by\n",
    "$$ \\mathrm{FIM}(a) = \\begin{bmatrix} C^{-1} & \\\\ & X \\end{bmatrix}$$\n",
    "where $X$ is a 4-th order tensor satisfying $X Y = \\frac{1}{4}C^{-1} (Y + Y^T) C^{-1}$ for all $ Y \\in \\R^{N_{\\theta}\\times N_{\\theta}}$.\n",
    "This gives the gradient flow \n",
    "$$ \\dot  m_t = - C_t\\mathbb{E}_{\\rho_{a_t}}[\\nabla \\Phi_R ], \\qquad\n",
    "\\dot  C_t = C_t - C_t \\mathbb{E}_{\\rho_{a_t}}[\\nabla^2  \\Phi_R ]C_t $$.\n",
    "### Gaussian Mixture Approximation\n",
    "For Gaussian mixture distribution family \n",
    "$$\\rho_a^\\mathrm{GM}(\\theta) = \\sum_{k=1}^{K} w_k \\mathcal{N}(\\theta; m_k, C_k)$$ \n",
    "where $a=[m_1, ..., m_k, ..., m_K, C_1. ..., C_k, ..., C_K, w_1, ..., w_k, ..., w_K]$. \n",
    "\n",
    "Considering the contrained optimization problem \n",
    "$$\\argmin_\\sigma \\frac{\\langle \\nabla_a \\text{KL}[\\rho_{a} \\Vert \\rho_{\\text{post}}] ,  \\sigma\\rangle}{\\sqrt{\\langle \\sigma,  \\mathrm{FIM}(a) \\sigma \\rangle}},\\quad s.t. \\sum_{k=1}^{K} \\sigma_{ \\dot{w}_k }=0,$$\n",
    "its KKT conditions to the following natural gradient flow: \n",
    "$$\n",
    "\\begin{bmatrix} \\dot{m}_{k} \\\\ \\dot{C}_{k} \\\\ \\dot{w}_{k}  \\end{bmatrix}\n",
    "  =-(\\mathrm{FIM}(a))^{-1}\n",
    "  \\begin{bmatrix}\n",
    "  w_k\\int \\mathcal{N}_k(\\theta) \\Bigl( \\nabla_{\\theta} \\log\\rho_a^\\mathrm{GM}  +\\nabla_{\\theta} \\Phi_R \\Bigr)  \\,\\mathrm{d}\\theta\n",
    "  \\\\\n",
    "  \\frac{w_k}{2}\\int \\mathcal{N}_k(\\theta) \\Bigl(\\nabla_{\\theta}\\nabla_{\\theta}\\log \\rho_a^\\mathrm{GM}  + \\nabla_{\\theta}\\nabla_{\\theta}\\Phi_R\\Bigr) \\,\\mathrm{d}\\theta\n",
    "  \\\\\n",
    "  \\int \\mathcal{N}_k(\\theta)\n",
    "  \\bigl(\n",
    "  \\log \\rho_a^\\mathrm{GM} + \\Phi_R \n",
    "  \\bigr) \\,\\mathrm{d}\\theta  + \\lambda\n",
    "  \\end{bmatrix}.\n",
    "$$\n",
    "where $\\mathcal{N}_k(\\theta)=\\mathcal{N}(\\theta;m_k, C_k)$ and $\\lambda$ is a Lagrangian multiplier determined by the constraint $\\sum_{k=1}^{K} \\dot{w}_k =0$.\n",
    "\n",
    "We approximate the Fisher information matrix by\n",
    "$$ \\mathrm{FIM}(a) \\approx \\textrm{diag}\\left(w_1 C_1^{-1}, ..., w_k C_k^{-1}, ..., w_K C_K^{-1}, w_1X_1, ..., w_kX_k, ..., w_KX_K, \\frac{1}{w_1}, ..., \\frac{1}{w_k}, ..., \\frac{1}{w_K}\\right),$$\n",
    "where $X_k$ is a 4-th order tensor satisfying $X_k Y = \\frac{1}{4}C_k^{-1} (Y + Y^T) C_k^{-1}$ for all $ Y \\in \\R^{N_{\\theta}\\times N_{\\theta}}$.\n",
    "\n",
    "Finally, we get the following equations:\n",
    "$$ \\begin{aligned}  \n",
    "\\frac{\\,\\mathrm{d} m_k}{\\,\\mathrm{d} t} &= -C_k\\int \\mathcal{N}_k(\\theta) \\Bigl( \\nabla_{\\theta} \\log\\rho_a^{\\rm GM}  +  \\nabla_{\\theta} \\Phi_R \\Bigr)  \\,\\mathrm{d}\\theta, \\\\\n",
    "\\frac{\\,\\mathrm{d} C_{k}^{-1}}{\\,\\mathrm{d} t} &=   \\int \\mathcal{N}_k(\\theta) \\bigl(\\nabla_{\\theta}\\nabla_{\\theta}\\log \\rho_a^{\\rm GM}  + \\nabla_{\\theta}\\nabla_{\\theta}\\Phi_R\\bigr) \\,\\mathrm{d}\\theta,\\qquad (1)  \\\\\n",
    "\\frac{\\,\\mathrm{d} \\log w_{k}}{\\,\\mathrm{d} t} &= -\\int \\Bigl(\\mathcal{N}_k(\\theta) -  \\rho_a^{\\rm GM}\\Bigr)\\bigl(\\log \\rho_a^{\\rm GM}  + \\Phi_R \\bigr) \\,\\mathrm{d}\\theta.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Derivative Free Gaussian Mixture Variational Inference\n",
    "We introduce a stable, derivative free approximation of (1) by the following quadrature rules.\n",
    "\n",
    "### Quadrature rule to compute $\\mathbb{E}_\\mathcal{N}[\\Phi_R],\\mathbb{E}_{\\mathcal{N}}[\\nabla_{\\theta}\\Phi_R],\\mathbb{E}_{\\mathcal{N}}[\\nabla_{\\theta}\\nabla_{\\theta}\\Phi_R]$\n",
    "Given a Gaussian density $\\mathcal{N}(\\theta)=\\mathcal{N}(\\theta;m,C)$, and a hyperparameter $\\alpha>0$, we first generate $2N_{\\theta}+1$ quadrature points\n",
    "$$ \\theta_0=m,\\ \\theta_i=m+\\alpha [\\sqrt{C}]_i,\\ \\theta_{N_\\theta+i}=m-\\alpha [\\sqrt{C}]_i \\ (1\\leq i \\leq N_\\theta) $$\n",
    "and then define vectors \n",
    "$$ c = \\mathcal{F}(\\theta_0), \\ b_i = \\frac{\\mathcal{F}(\\theta_i) - \\mathcal{F}(\\theta_{N_\\theta + i})}{2\\alpha}, \\\n",
    "    a_i = \\frac{\\mathcal{F}(\\theta_i) + \\mathcal{F}(\\theta_{N_\\theta + i}) - 2\\mathcal{F}(\\theta_0)}{2\\alpha^2} \\ (1 \\leq i \\leq N_\\theta), $$\n",
    "and denote $B=[b_1,b_2,\\dots,b_{N_\\theta}],\\,A=[a_1,a_2,\\dots,a_{N_\\theta}]$.\n",
    "\n",
    "We approximate the expectation of the function, the gradient, the Hessian as\n",
    "$$\\begin{aligned}\n",
    "    \\mathbb{E}_{\\mathcal{N}}[\\Phi_R ]  & \\approx \\frac{1}{2}  c^Tc, \\\\\n",
    "    \\mathbb{E}_{\\mathcal{N}}[\\nabla_\\theta   \\Phi_R ]  & \\approx \\sqrt{C}^{-T}  B^Tc, \\\\\n",
    "    \\mathbb{E}_{\\mathcal{N}}[\\nabla_\\theta \\nabla_\\theta  \\Phi_R ]  & \\approx \\sqrt{C}^{-T}(6\\mathrm{Diag}(A^TA) + B^T B)\\sqrt{C}^{-1}.\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Quadrature rule to compute $\\mathbb{E}_{\\mathcal{N}_k}[\\log\\rho_a^{\\rm GM}(\\theta)],\\mathbb{E}_{\\mathcal{N}_k}[\\nabla_\\theta \\log\\rho_a^{\\rm GM}(\\theta)],\\mathbb{E}_{\\mathcal{N}_k}[\\nabla_\\theta\\nabla_\\theta\\log\\rho_a^{\\rm GM}(\\theta)]$\n",
    "For the expecation of $\\log\\rho_a^{\\rm GM}(\\theta)$ and its gradient, we use mean-point approximation:\n",
    "$$ \\begin{aligned}\n",
    "    \\mathbb{E}_{\\mathcal{N}_k}[\\log\\rho_a^{\\rm GM}(\\theta)]  \\approx & \\log\\rho_a^{\\rm GM}(m_k) \\\\\n",
    "    \\mathbb{E}_{\\nabla_\\theta\\mathcal{N}_k}[\\log\\rho_a^{\\rm GM}(\\theta)] \\approx & \\nabla_\\theta \\log\\rho_a^{\\rm GM}(m_k).\n",
    "\\end{aligned}\n",
    "$$\n",
    "The expectation of the Hessian is approximated as \n",
    "$$\n",
    "    \\mathbb{E}_{\\nabla_\\theta\\nabla_\\theta\\mathcal{N}_k}[\\log\\rho_a^{\\rm GM}(\\theta)] \\approx\n",
    "    -C_k^{-1} + \\frac{\\sum_{i<j} w_iw_j \\bigl(v_i(m_k) - v_j(m_k)\\bigr)\\bigl(v_i(m_k) - v_j(m_k)\\bigr)^T\\mathcal{N}_i(m_k)\\mathcal{N}_j(m_k)}{ \\rho_a^{\\rm GM}(m_k)^2}.\n",
    "$$\n",
    "where $v_i(\\theta) = C_i^{-1}(\\theta - m_i)$.\n",
    "\n",
    "\n",
    "Finally, we update the covariances, means, and weights sequentially using a forward Euler scheme as follows:\n",
    "$$\\begin{aligned}\n",
    "C_{k}^{-1}(t+\\Delta t) \n",
    "        &= C_{k}^{-1}(t)  + \\Delta t ~ \\mathrm{\\mathsf{QR}}_{\\N_k(t)}\\bigl\\{\\nabla_{\\theta}\\nabla_{\\theta}\\log \\rho_a^{\\rm GM}(t) + \\nabla_{\\theta}\\nabla_{\\theta}\\Phi_R\\bigr\\},\n",
    "        \\\\\n",
    "        m_{k}(t+\\Delta t) \n",
    "        &= m_{k}(t)  - \\Delta t ~ C_k(t+\\Delta t) ~ \\mathrm{\\mathsf{QR}}_{\\N_k(t)} \\bigl\\{\\nabla_{\\theta} \\log\\rho_a^{\\rm GM}(t)  +  \\nabla_{\\theta} \\Phi_R \\bigr\\}, \\qquad(2)\n",
    "        \\\\\n",
    "        \\log w_{k}(t+\\Delta t) &= \\log w_{k}(t) - \\Delta t ~ \\mathrm{\\mathsf{QR}}_{\\N_k(t)} \\bigl\\{\\log \\rho_a^{\\rm GM}(t)  + \\Phi_R \\bigr\\}. \n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $QR\\{\\cdot\\}$ represents quadrature rules above. We then normalize $w_k(t + ∆t)_{k=1}^k$ and, for efficiency, set a lower bound of $w_k$ at a default value of $10^{−8}$ during normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Theoretical Analysis\n",
    "There are two theoretical properties of the natural gradient flow defined by (1) and the proposed DF-GMVI algorithm described in (2).\n",
    "* For the DF-GMVI algorithm described in (2), if $0 < \\Delta t < 1$, then $C_k$ remains positive definite. \n",
    "* Affine invariance (consider an affine mapping $\\varphi: \\theta \\to T\\theta+b$ below)\n",
    "    * The natural gradient flow defined by (1) is affine invariant for any invertible matrix $T$.\n",
    "    * The proposed DF-GMVI algorithm described in (2) is affine invariant for any invertible lower triangular matrix $T$, when using Cholesky decomposition to compute the square root matrix $\\sqrt{C}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2D Numercial Example\n",
    "In this part, we apply the DF-GMVI algorithm to approximate the distribution that follows the Rosenbrock function, which has a character\n",
    "istic “banana” shape. That is, the posterior $\\rho_{\\text{post}}(\\theta)\\propto \\exp(-\\frac{1}{2}\\mathcal{F}(\\theta)^T\\mathcal{F}(\\theta))$, with \n",
    "$$ \n",
    "\\mathcal{F}(\\theta) = \n",
    "\\frac{1}{\\sqrt{10}}\\Bigl(y - \\begin{bmatrix}\n",
    "10(\\theta_{(2)} -  \\theta_{(1)}^2)\\\\\n",
    "\\theta_{(1)}\n",
    "\\end{bmatrix}\\Bigr)\n",
    "\\quad \\textrm{ and }\\quad \n",
    "y= \n",
    "\\begin{bmatrix}\n",
    "0\\\\\n",
    "1\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "We apply DF-GMVI with $K = 10,20$, and $40$ modes, each randomly initialized as $\\mathcal{N}(0,I)$ with equal weights. We set the hyperparameter $\\alpha=10^{-3}$, the step size $\\Delta t=0.5$. We run the algorithm for 200 iterations. The density estimations at the 200th iteration and the errors in terms of total variation over the iterations are shown in the following pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "using Random\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using ForwardDiff\n",
    "include(\"../Inversion/Plot.jl\")\n",
    "include(\"../Inversion/DF_GMVI.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "outputs": [],
   "source": [
    "function func_F(x)\n",
    "    F1 = 0-10*(x[2]-x[1]^2)\n",
    "    F2 = 1-x[1]\n",
    "    return [F1,F2]/sqrt(10)\n",
    "end\n",
    "\n",
    "N_modes_array = [10; 20; 40]\n",
    "\n",
    "fig, ax = PyPlot.subplots(nrows=1, ncols=length(N_modes_array)+2, sharex=false, sharey=false, figsize=(20,5))\n",
    "\n",
    "\n",
    "N_modes = N_modes_array[end]\n",
    "x0_w  = ones(N_modes)/N_modes\n",
    "μ0, Σ0 = [0.0; 0.0], [1.0 0.0; 0.0 1.0]\n",
    "N_x = length(μ0)\n",
    "x0_mean, xx0_cov = zeros(N_modes, N_x), zeros(N_modes, N_x, N_x)\n",
    "for im = 1:N_modes\n",
    "    x0_mean[im, :]    .= rand(MvNormal(zeros(N_x), Σ0)) + μ0\n",
    "    xx0_cov[im, :, :] .= Σ0\n",
    "end\n",
    "\n",
    "\n",
    "N_iter = 200\n",
    "Nx, Ny = 100,100\n",
    "dt = 5e-1\n",
    "\n",
    "μ0, Σ0 = [0.0; 0.0], [1.0 0.0; 0.0 1.0]\n",
    "objs = [Gaussian_mixture_VI(nothing, func_F, x0_w[1:N_modes], x0_mean[1:N_modes,:], xx0_cov[1:N_modes,:,:]; N_iter = N_iter, dt = dt)[1] \n",
    "        for N_modes in N_modes_array]\n",
    "visualization_2d(ax ; Nx = Nx, Ny = Ny, x_lim=[-4.0, 4.0], y_lim=[-2.0, 10.0], func_F=func_F, objs=objs)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"DF-GMVI-2D-Example.pdf\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
